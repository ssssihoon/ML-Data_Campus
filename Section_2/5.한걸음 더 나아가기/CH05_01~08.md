# CH05
- [CH05](#ch05)
- [1. 머신러닝 소개](#1-머신러닝-소개)
- [2. 데이터 전처리](#2-데이터-전처리)
  - [필요없는 필드 삭제](#필요없는-필드-삭제)
  - [성별 인코딩](#성별-인코딩)
    - [나이분류](#나이분류)
- [3. 머신러닝 모델 구성 및 결과 검증](#3-머신러닝-모델-구성-및-결과-검증)
    - [모델 생성 및 학습](#모델-생성-및-학습)
    - [모델 정확도 검증](#모델-정확도-검증)
    - [실제값 예측값 비교 구현](#실제값-예측값-비교-구현)
- [4. 다양한 머신러닝 기술](#4-다양한-머신러닝-기술)
  - [배깅 (랜덤 포레스트)](#배깅-랜덤-포레스트)
  - [부스팅](#부스팅)
- [5. 딥러닝 소개](#5-딥러닝-소개)
- [6. numpy를 이용한 행렬 연산](#6-numpy를-이용한-행렬-연산)
  - [행렬 형태 변경](#행렬-형태-변경)
  - [행렬간 사칙연산](#행렬간-사칙연산)
    - [행렬의 형태가 다른 경우는 불가](#행렬의-형태가-다른-경우는-불가)
    - [행렬의 곱셈](#행렬의-곱셈)
- [7. 딥러닝 모델 구성 및 결과 검증](#7-딥러닝-모델-구성-및-결과-검증)
    - [정규화](#정규화)
    - [그림 그리기](#그림-그리기)
  - [모델 작성](#모델-작성)
    - [모델 시각화](#모델-시각화)
    - [모델 학습](#모델-학습)
    - [모델 평가](#모델-평가)

# 1. 머신러닝 소개

인공지능 ∈ 머신러닝 ∈ 딥러닝

# 2. 데이터 전처리

```python
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

df = sns.load_dataset('titanic')
print(df.head())
```

```python
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

df = sns.load_dataset('titanic')
train_df = df[:800]
test_df = df[800:]
# 학습할 데이터와 실험할 데이터를 선언
# 0~800 -> 학습 데이터로 사용
# 800~891 -> 테스트 할 데이터로 사용
print(len(train_df))
print(len(test_df))

'''
800
91
'''
```

- pclasss(등급) 와 survived(생존률) 관계

mean() →평균값

```python
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

df = sns.load_dataset('titanic')
train_df = df[:800]
test_df = df[800:]

df1 = train_df[['pclass', 'survived']].groupby(['pclass'], as_index=False).mean().sort_values(by='survived', ascending=False)
print(df1)

'''
   pclass  survived
0       1  0.615385
1       2  0.481928
2       3  0.246014
'''
```

- pclass에 따른 age별 survived 유무

```python
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

df = sns.load_dataset('titanic')
train_df = df[:800]
test_df = df[800:]

df1 = train_df[['age', 'survived']].groupby(['age'], as_index=False).mean().sort_values(by='survived', ascending=False)
a = sns.FacetGrid(train_df, col='survived', row = 'pclass')
a.map(plt.hist, 'age', bins = 20)
print(plt.show())
```

![iShot_2023-07-20_06 26 17](https://github.com/ssssihoon/ML-Data_Campus/assets/127017020/3bb71bb4-fffc-43f5-9e3f-78b760d36a53)


## 필요없는 필드 삭제

- 항목들 확인

```python
names = train_df.columns
print(names)

'''
Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',
       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',
       'alive', 'alone'],
      dtype='object')
'''
```

- 삭제 (drop)

```python
names = train_df.columns
train_df = train_df.drop(names[4::], axis = 1)
test_df = test_df.drop(names[4::], axis = 1)

'''
survived  pclass     sex   age
0           0       3    male  22.0
1           1       1  female  38.0
2           1       3  female  26.0
3           1       1  female  35.0
4           0       3    male  35.0
..        ...     ...     ...   ...
795         0       2    male  39.0
796         1       1  female  49.0
797         1       3  female  31.0
798         0       3    male  30.0
799         0       3  female  30.0

[800 rows x 4 columns]
     survived  pclass     sex    age
800         0       2    male  34.00
801         1       2  female  31.00
802         1       1    male  11.00
803         1       3    male   0.42
804         1       3    male  27.00
..        ...     ...     ...    ...
886         0       2    male  27.00
887         1       1  female  19.00
888         0       3  female    NaN
889         1       1    male  26.00
890         0       3    male  32.00
'''
```

- 결측값 확인

```python
print(train_df.isnull().sum())
print(test_df.isnull().sum())

'''
survived      0
pclass        0
sex           0
age         163
dtype: int64
survived     0
pclass       0
sex          0
age         14
dtype: int64
'''
```

- age 평균으로 age 결측값 채우기

```python
train_df.fillna(train_df.mean()[['age']], inplace=True)
test_df.fillna(test_df.mean()[['age']], inplace=True)
```

## 성별 인코딩

```python
map_dict = {'female' : 0, 'male' : 1}

train_df['sex'] = train_df['sex'].map(map_dict).astype(int)
test_df['sex'] = test_df['sex'].map(map_dict).astype(int)
```

### 나이분류

```python
def function1(x):
	if x < 20:
		return 1
	elif x <40:
		return 2
	elif x < 50:
		return 3
	else :
		return 4
train_df['age'] = train_df['age'].apply(function1)
test_df['age'] = test_df['age'].apply(function1)
```

# 3. 머신러닝 모델 구성 및 결과 검증

- 데이터 분류

```python
x_train = train_df.drop(['survived'], axis = 1)
y_train = train_df['survived']
# x train : 문제 , y train : 답
x_test = test_df.drop(['survived'], axis = 1)
y_test = test_df['survived']
#복제
```

### 모델 생성 및 학습

- Decision tree

```python
decision_tree = DecisionTreeClassifier()
decision_tree.fit(x_train, y_train)
# x(문제)와 y(정답)을 학습시키겠다
```

### 모델 정확도 검증

```python
print(decision_tree.score(x_train, y_train))
print(decision_tree.score(x_test, y_test))

'''
0.8
0.7692307692307693
'''
train x와 y의 값의 정확도
test x와 y의 값의 정확도 출력
```

### 실제값 예측값 비교 구현

```python
y_pred = decision_tree.predict(x_test)
print(y_pred)

'''
[0 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0 0 0 1 0
 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 1 1 1 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 1 0 0
 1 1 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0]
'''
0 -> survived
1 -> un ''

```

```python
y_test_list = list(y_test)
# y_pred가 리스트 형식이므로 시리즈 형식인 y_test를 리스트 형식으로 바꿔줌
```

```python
total = 0
for i in range(len(y_pred)):
    if y_pred[i] == y_test_list[i]:
        total += 1
    else:
        pass
print(total)
print(total / len(y_pred))

'''
70
0.7692307692307693 -> score 함수와 같음
'''
```

# 4. 다양한 머신러닝 기술

## 배깅 (랜덤 포레스트)

여러가지 모델들을 편향되지 않게 결과를 도출함

```python
from sklearn.ensemble import RandomForestClassifier
random_forest = RandomForestClassifier(n_estimators=100)
random_forest.fit(x_train, y_train)
print(random_forest.score(x_train, y_train))
print(random_forest.score(x_test, y_test))

'''
0.8
0.7692307692307693
'''
위 score 함수와 비교해보면 정확도값이 좋아짐(좋은쪽으로)
```

## 부스팅

모델의 결점을 보완해서 새 모델을 만듦을 반복 후 결과 도출

```python
import xgboost as xgb
boosting_model = xgb.XGBClassifier(n_estimators = 100)
boosting_model.fit(x_train, y_train)
print(boosting_model.score(x_train, y_train))
print(boosting_model.score(x_test, y_test))

'''
0.79875
0.7802197802197802
'''
```

캐글에서 많이 쓰인다.

# 5. 딥러닝 소개

- 데이터를 가중치(w)를 적용해 결과값을 도출한다.
- 행렬을 많이 이용

# 6. numpy를 이용한 행렬 연산

```python
import numpy as np

a = np.array([[1, 2], [3, 4]])
print(a)

'''
[[1 2]
 [3 4]]
'''
```

```python
import numpy as np

import numpy as np

a = np.array([[1, 2, 3], [4, 5, 6]])
print(a)

'''
[[1 2 3]
 [4 5 6]]
'''
a = np.array([[1, 2, 3], [4, 5, 6]])
print(a.T)

'''
[[1 4]
 [2 5]
 [3 6]]
'''
.T를 이용하여 행렬 형태변환

```

## 행렬 형태 변경

```python
import numpy as np

a = np.array([[1, 2, 3], [4, 5, 6]])
b = np.reshape(a, (3, 2))

print(b.shape)
print(b)

'''
(3, 2) # 3행 2열

[[1 2]
 [3 4]
 [5 6]]
'''
```

## 행렬간 사칙연산

```python
import numpy as np

a = np.array([[1, 2, 3], [4, 5, 6]])
b = np.array([[2, 3, 4], [5, 6, 7]])

print(a+b)

'''
[[ 3  5  7]
 [ 9 11 13]]
'''
```

### 행렬의 형태가 다른 경우는 불가

- Value Error

### 행렬의 곱셈

```python
import numpy as np

a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

print(np.dot(a, b))

'''
32
'''
# 1*4 2*5 3*6 -> sum -> 32
```

# 7. 딥러닝 모델 구성 및 결과 검증

```python
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

print(len(x_train), len(y_train))
print(len(x_test), len(y_test))

'''
60000 60000
10000 10000
'''
```

```python
print(x_train[0].shape)

'''
(28, 28)
'''
```

### 정규화

정확도를 높이기 위해 사용함

255에 편향 될 가능성이 크기 때문에 255로 나눔

```python
x_train, x_test = x_train / 255.0, x_test / 255.0
```

### 그림 그리기

```python
plt.imshow(x_test[0])
print(plt.show())
```

![iShot_2023-07-20_10 08 23](https://github.com/ssssihoon/ML-Data_Campus/assets/127017020/a57a9e34-7176-42db-b75c-50de192e92ff)


```python
print(y_test[0])

'''
7
'''
```

## 모델 작성

```python
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
# input 데이터 28*28을 받겠다.
  tf.keras.layers.Dense(128, activation='relu'),
# 128개로 이루어진 히든레이어를 만들겠다.
  tf.keras.layers.Dropout(0.2),
# 20퍼센트 이하의 데이터 입력이 나오더라도 다시 학습해라
  tf.keras.layers.Dense(10, activation='softmax')
])
# 출력은 0~9 까지 분류

model.compile(optimizer='adam',
# 학습의 효율을 최적화
              loss='sparse_categorical_crossentropy',
# 실제 값과 예측 값의 차이를 loss -> 오차를 어떻게 줄일 것인가
              metrics=['accuracy'])
# 정확도가 얼마나 높아지는지
```

### 모델 시각화

```python
model.summary()

'''
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 flatten (Flatten)           (None, 784)               0         
                                                                 
 dense (Dense)               (None, 128)               100480    
                                                                 
 dropout (Dropout)           (None, 128)               0         
                                                                 
 dense_1 (Dense)             (None, 10)                1290      
                                                                 
=================================================================
Total params: 101770 (397.54 KB)
Trainable params: 101770 (397.54 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
'''
```

### 모델 학습

```python
model.fit(x_train, y_train, epochs = 5)
# epochs -> 몇 번 반복할 것인지

'''
Epoch 1/5
1875/1875 [==============================] - 1s 543us/step - loss: 0.2995 - accuracy: 0.9132
Epoch 2/5
1875/1875 [==============================] - 1s 541us/step - loss: 0.1470 - accuracy: 0.9567
Epoch 3/5
1875/1875 [==============================] - 1s 537us/step - loss: 0.1080 - accuracy: 0.9673
Epoch 4/5
1875/1875 [==============================] - 1s 538us/step - loss: 0.0901 - accuracy: 0.9718
Epoch 5/5
1875/1875 [==============================] - 1s 548us/step - loss: 0.0770 - accuracy: 0.9754
<keras.src.callbacks.History object at 0x166688490>
'''
```

### 모델 평가

```python
print(model.evaluate(x_test,  y_test, verbose = 2))

'''
313/313 - 0s - loss: 0.0784 - accuracy: 0.9751 - 140ms/epoch - 447us/step
[0.0783534124493599, 0.9750999808311462]
'''
```