# CH02

- [1. Linear Regression](#1-linear-regression)
    - [데이터](#데이터)
    - [계수 (b = 베타)](#계수-b--베타)
    - [Mean Squared Error](#mean-squared-error)
- [2. Linear Regression 심화](#2-linear-regression-심화)
  - [Multivariate Regression](#multivariate-regression)
    - [다변량 회귀 수식](#다변량-회귀-수식)
  - [Polynomial Regression](#polynomial-regression)
    - [다항식 회귀 수식](#다항식-회귀-수식)
  - [회귀 계수를 계산하는 방법](#회귀-계수를-계산하는-방법)
    - [통계적 방법](#통계적-방법)
    - [ML 방법](#ml-방법)
- [3. Linear Regression 실습 (Python)](#3-linear-regression-실습-python)
  - [Univariate Regression](#univariate-regression)
    - [Data 변환](#data-변환)
  - [Linear Regression](#linear-regression)
    - [학습](#학습)
    - [모델의 식 확인](#모델의-식-확인)
  - [Multivariate Regression](#multivariate-regression-1)
    - [Sample Data](#sample-data)
    - [Multivariate Regression](#multivariate-regression-2)
    - [회귀식 확인](#회귀식-확인)
  - [통계적 방법](#통계적-방법-1)
  - [Polynomial Regression](#polynomial-regression-1)
    - [학습하기](#학습하기)
    - [회귀식 확인하기](#회귀식-확인하기)
    - [예측값 확인하기](#예측값-확인하기)
    - [예측값 plot로 확인하기](#예측값-plot로-확인하기)
- [4. Linear Regression 실습 (R)](#4-linear-regression-실습-r)
    - [모델 생성](#모델-생성)
    - [모델 확인](#모델-확인)
    - [예측](#예측)
  - [Multivariate Regression](#multivariate-regression-3)
    - [모델 생성](#모델-생성-1)
  - [통계적 방법](#통계적-방법-2)
  - [Polynomial Regression](#polynomial-regression-2)
    - [Simple Data](#simple-data)
    - [학습](#학습-1)
- [5. Regularization](#5-regularization)
  - [Overfitting을 방지하는 법](#overfitting을-방지하는-법)
- [6. 당뇨병 진행도 예측 (Python)](#6-당뇨병-진행도-예측-python)
- [7. 당뇨병 진행도 예측 (R)](#7-당뇨병-진행도-예측-r)

# 1. Linear Regression

선형회귀

**예측값을 직선으로 표현하는 모델**

예측값 = 편향 + 계수*입력값

### 데이터

- 예측값(prediction) y
- 입력값(data) x

### 계수 (b = 베타)

- 편향 b0
- 계수 b1 (Coefficient)

실제 값과 예측 값의 차이가 적은 직선이 좋음 (Y - Y^)

### Mean Squared Error

차이의 크기를 알아보려면 (y-y^)을 제곱하고 평균을 구한다.

# 2. Linear Regression 심화

## Multivariate Regression

다변량 회귀

- 두 개 이상의 변수로 만든 회귀식

### 다변량 회귀 수식

- y = b0 + b1x1 + b2x2 ~

Ex

몸무게를 이용해 키를 예측 → 키 = b0 + b1*몸무게

몸무게와 나이를 이용해 키를 예측 → 키 = b0 + b1*몸무게 + b2*나이

## Polynomial Regression

다항식 회귀

- 예측하는 값이 선형이 아닌 비선형(곡선)일 경우 사용

### 다항식 회귀 수식

- y = b0 + b1x + b2(x^2)

## 회귀 계수를 계산하는 방법

- 통계적 방법
- ML 방법

### 통계적 방법

- 최소제곱법

### ML 방법

여러 값을 대입 해 loss가 작은 b를 찾는다

- Bisection Method
    
    임의의 두개의 값을 설정 후 두 값의 y 값을 비교
    
    y값이 큰 점을 두점의 가운데 점으로 바꾼다
    
    임의의 두 값의 차이가 작아질 때 까지 계속 반복
    
- Gradient Descent (경사 하강법)
    
    임의의 값 하나를 설정 후 기울기를 계산
    
    기울기와 Learning rate를 곱한 값을 임의의 값에서 뺀다
    
    기울기가 0에 가까워질 때까지 계속 반복
    

# 3. Linear Regression 실습 (Python)

## Univariate Regression

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

np.random.random(2021)

x = np.array([1, 2, 3, 4])
y = np.array([2, 1, 4, 3])
plt.scatter(x, y)
print(plt.show())
```

### Data 변환

scikit - learn에서

- (n,c)의 형태로 되어 있어야 함. n = 데이터 개수, c 는 feature의 개수

```python
data = x.reshape(-1, 1)
print(data)
print(data.shape)

'''
[[1]
 [2]
 [3]
 [4]]

(4, 1)
'''
```

## Linear Regression

### 학습

- `model = LinearRegression()`
- model 학습은 fit 함수를 통해 가능
- model.fit(x = …, y = ….) → x : 학습에 사용할 데이터, y : 학습에 사용할 정답

```python
model.fit(x = data, y = y)
or
mdel.fit(data, y)
```

### 모델의 식 확인

1. bias 편향 확인
    
    sklearn 에서 intercept_로 확인
    

```python
print(model.intercept_)

'''
1.0000000000000004
'''
```

1. 회귀계수 확인
    
    coef_로 확인
    

```python
print(model.coef_)

'''
[0.6]
'''
```

다음으로 결과 값은? (회귀선)

y = 1.0000000000000004 + 0.6*x      → 회귀선

1. 예측하기
    
    predict 함수를 통해 확인
    

```python
pred = model.predict(data)
print(pred)

'''
[1.6 2.2 2.8 3.4]
'''
```

1. 회귀선으로 표현

```python
plt.scatter(x, y)
plt.plot(x, pred, color = "green")
print(plt.show())
```

![iShot_2023-08-01_18 10 59](https://github.com/ssssihoon/CodingTest_Algorithm/assets/127017020/98305931-32e0-476c-a1a4-5713147384f6)


## Multivariate Regression

### Sample Data

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

bias = 1
beta = np.array([2, 3, 4, 5]).reshape(4, 1)
noise = np.random.randn(100, 1)

x = np.random.randn(100, 4)
y = bias + x.dot(beta)
y_with_noise = y + noise     #noise값이 있다고 가정

print(x[10])
print(y[10])

'''
[ 0.18044281  1.27292463 -1.18067227  0.20994747]
[1.50670779]
'''
```

### Multivariate Regression

학슴

```python
model = LinearRegression()
model.fit(x, y_with_noise)
```

### 회귀식 확인

```python
print(model.intercept_) #편향값
print(model.coef_) #회귀계수

'''
[1.10684591]
[[1.96110905 2.8826797  3.77370857 5.02573308]]
'''
결론 : 편향값은 잘 못맞추지만, 회귀 계수의 경우 정확하게 예측 가능
```

## 통계적 방법

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

bias = 1
beta = np.array([2, 3, 4, 5]).reshape(4, 1)
noise = np.random.randn(100, 1)

x = np.random.randn(100, 4)
y = bias + x.dot(beta)
y_with_noise = y + noise

bias_x = np.array([1]*len(x)).reshape(-1, 1)
stat_x = np.hstack([bias_x ,x])
x_x_transpose = stat_x.transpose().dot(stat_x)
x_x_transpose_inverse = np.linalg.inv(x_x_transpose)
stat_beta = x_x_transpose_inverse.dot(stat_x.transpose()).dot(y_with_noise)
print(stat_beta)

'''
[[0.92669819]
 [1.76722627]
 [2.98501453]
 [4.07105479]
 [5.09206952]]
'''
```

## Polynomial Regression

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

bias = 1
beta = np.array([2, 3]).reshape(2, 1)
noise = np.random.randn(100, 1)

x = np.random.randn(100, 1)
x_poly = np.hstack([x, x**2])

y = bias + x_poly.dot(beta)
y_with_noise = y + noise
plt.scatter(x, y_with_noise)
print(plt.show())
```

![iShot_2023-08-01_18 29 44](https://github.com/ssssihoon/CodingTest_Algorithm/assets/127017020/a4bb6574-5f9d-4c5e-a262-e3a7dc1fd733)


### 학습하기

```python
model = LinearRegression()
model.fit(x_poly, y_with_noise)
```

### 회귀식 확인하기

```python
print(model.intercept_)
print(model.coef_)

'''
[1.32349159]
[[2.09858356 2.70360616]]
'''
```

### 예측값 확인하기

```python
pred = model.predict(x_poly)
print(pred)

'''
[[ 1.43829868]
 [ 5.98976384]
 [ 9.96237822]
 [ 4.15455145]
 [ 3.09100361]
~~~~
'''
```

### 예측값 plot로 확인하기

```python
plt.scatter(x, pred)
print(plt.show())
```

![iShot_2023-08-01_18 34 40](https://github.com/ssssihoon/CodingTest_Algorithm/assets/127017020/3a4a68de-5993-4ea1-981b-cc2e80b0d5ae)


# 4. Linear Regression 실습 (R)

- 패키지, 라이브러리

```r
install.packages("tidyverse")
install.packages("tidymodels")

library("tidyverse")
library("tidymodels")
rm(list-ls())
set.seed(2021)
```

```r
x = c(1, 2, 3, 4)
y = c(2, 1, 4, 3)
data = tibble(x = x, y = y)
data

'''
    x     y
<dbl> <dbl>
1     1     2
2     2     1
3     3     4
4     4     3
```

- plot으로 그리기

```r
ggplot() +  geom_point(data-data, mapping = aes(x = x, y = y))
```

### 모델 생성

```r
linear_regression = linear_reg() %>%
   fit(y~x, data = data)   #예측하려는 변수 ~ 예측에 이용하려는 변수
```

### 모델 확인

```r
linear_regression %>%
   extract_fit_engine() %>%
   coef

'''
(Intercept)           x 
        1.0         0.6
'''
```

### 예측

```r
pred = predict(linear_regression, data)
pred

'''
.pred
  <dbl>
   1.62
   2.23
   2.84
    3.4
```

## Multivariate Regression

sample data

```r
bias = 1
beta = c(2, 3, 4, 5)
noise = rnorm(100)
x = matrix(rnorm(400), ncol = 4)
y = bias + x %*% beta
y_with_noise = y + noise
data = cbind(
  as_tibble(x),
  tibble(y = y_with_noise)
)
data

```

### 모델 생성

```r
multivariate_regression = linear_reg() %>%
   fit(y~., data = data)multivariate_regression %>% extract_fit_engine() %>% coef

'''
(Intercept)          V1          V2          V3          V4
    1.167149    1.904347    2.880201    3.956052    5.040586
```

## 통계적 방법

```r
bias_x = rep.int(1, 100)
stat_x = cbind(bias_x, x)
x_x_transpose = t(stat_x) %*% stat_x
x_x_transpose_inverse = solve(x_x_transpose)
stat_beta = x_x_transpose_inverse %*% t(stat_x) %*% y_with_noise
stat_beta

'''
          [,1]
bias_x 1.167149
       1.904347
       2.880201
       3.956052
       5.040586
```

## Polynomial Regression

### Simple Data

```r
bias = 1
beta = c(2, 3)
noise = rnorm(100)
x = rnorm(100)
x_poly = cbind(x, x**2)
y = bias + as.matrix(x_poly) %*% beta
y_with_noise = y + noise
data = cbind(as_tibble(x_poly), tibble(y=y_with_noise))
data

ggplot() +
  geom_point(data = data, aes(x=x, y=y))
```

데이터 시각화를 위해 tibble 처리

![iShot_2023-08-01_20 57 03](https://github.com/ssssihoon/CodingTest_Algorithm/assets/127017020/42ff4e78-a4ba-4d37-96e5-bf3475bba9b9)


### 학습

```r
colnames(data)

'''
"x"  "V2" "y"
```

```r
poly_regression = linear_reg() %>%
   fit(y~x + V2, data = data)

poly_regression %>% extract_fit_engine() %>% coef
'''
(Intercept)           x          V2
    1.030324    1.953825    3.009977
```

```r
poly_regression = linear_reg() %>%
   fit(y~., data= data) # 모든 변수를 뜻하는 (.)
```

# 5. Regularization

## Overfitting을 방지하는 법

- 더 많은 학습 데이터
- 모델의 정규화 : 모델에 제한을 줌

# 6. 당뇨병 진행도 예측 (Python)

```python
from sklearn.datasets import load_diabetes

diabetes = load_diabetes()

print(diabetes["feature_names"])

'''
['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']
'''
```

```python
data, target = diabetes["data"], diabetes["target"]
print(data[0])
print(target[0])

'''
[ 0.03807591  0.05068012  0.06169621  0.02187239 -0.0442235  -0.03482076
 -0.04340085 -0.00259226  0.01990749 -0.01764613]
151.0
'''
```

# 7. 당뇨병 진행도 예측 (R)
