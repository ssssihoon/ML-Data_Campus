# 머신러닝을 위한 기초수학

# 미적분학

# CH02_06 다변함수와 그래디언트

f(x;θ) = θx ~> argminθf(x;θ)

→ θ에 따라 값이 달라지는 모델 ~> 최적화값을 찾겠다.

**편미분을 활용**

편미분은 f(x1, x2) = x1 + x2일 때, 

x1에 대해서 편미분을 할 경우 라운드 기호를 활용 ∂ (x2는 상수취급)

∂f/∂x1 = 1 + 0이 된다.

f(x1, x2) = x1*x2

편미분 → ∂f/∂x1 = x2 + 0

### Multivariate Functions → 이것을  계속 편미분 한 값을 활용해 최적화 값을 구함

x1 := x1 - a*∂f/∂x1

x2 := x2 - a*∂f/∂x2

### Gradients

x1, x2에 대한 편미분을 모아둔 것을 Gradients라고 한다. ∂f/∂x

## Gradient-based Optimization

위의 식들을 활용 함.

x := x - a∇xf(x)

→     xn := xn - a*∂f/∂xn

1 ~ n 까지를 모아 둔 것을 Gradient-based Optimization이라고 함.

**∇ : gradient (편미분)**

# **CH02_07 인공뉴런의 최적화**

## Optimizations of Artificial Neurons (인공뉴런의 최적화)

인공 뉴런의 경우 두개의 함수로 되어있음

합성함수로 되어 있는데, 벡터의 입력 세타(파라미터)을 x로 받아 변수 a로 활용

x벡터 → V(x벡터 ; θ벡터){편미분} → a

Z = θn*xn + ‘’’ + θ1*x1 + θ0*x0

   = a(1-a)xn + ‘’’ + a(1-a)*x1 + a(1-a)

********************************체인룰에 대해 다시한번 복기할 것.********************************

딥러닝과 CNN에 활용됨

# **CH02_08 벡터하무와 그래디언트**

## Vector Functions and Gradients (벡터함수와 편미분)

******f******(x) (**f** = 벡터를 두꺼운 글씨로 표현)

**f**(x) = f1(x), f2(x), f3(x) ‘’’ 

f1(x) → ∂f1/∂x

f2(x) → ∂f2/∂x

‘’’

fn(x) → ∂fn/∂x

# **CH02_09 야코비안과 체인룰**

## Multivariate and Vector Functions

f1(**X**) ****= y1

f2(**X**) ****= y2

f3(**X**) ****= y3

‘’’

## General Jacobians

Gradients는 가로로 길게 써주는 식.

벡터function에 대한 미분은 세로로 써줘서 하나씩 편미분함

# **CH02_10 구분구적법과 정적법**

******************************아는 내용 SKIP******************************

# **CH02_11 적분공식과 부정적분**

******************************아는 내용 SKIP******************************